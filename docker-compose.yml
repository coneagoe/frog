# common variable & volume
x-postgres-common-env: &postgres-common-env
  POSTGRES_DB: quant
  POSTGRES_USER: quant
  POSTGRES_PASSWORD: quant

x-airflow-common-depends-on: &airflow-common-depends-on
  db:
    condition: service_healthy
  redis:
    condition: service_started
  db-init-airflow:
    condition: service_completed_successfully
  airflow-init-permissions:
    condition: service_completed_successfully

x-airflow-email-env: &airflow-email-env
  # Email (required; define these in .env)
  AIRFLOW__SMTP__SMTP_HOST: ${SMTP_HOST:?SMTP_HOST is required}
  AIRFLOW__SMTP__SMTP_PORT: ${SMTP_PORT:?SMTP_PORT is required}
  AIRFLOW__SMTP__SMTP_USER: ${SMTP_USER:?SMTP_USER is required}
  AIRFLOW__SMTP__SMTP_PASSWORD: ${SMTP_PASSWORD:?SMTP_PASSWORD is required}
  AIRFLOW__SMTP__SMTP_MAIL_FROM: ${SMTP_MAIL_FROM:?SMTP_MAIL_FROM is required}
  AIRFLOW__SMTP__SMTP_SSL: "true"
  AIRFLOW__SMTP__SMTP_STARTTLS: "false"
  ALERT_EMAILS: ${ALERT_EMAILS:?ALERT_EMAILS is required}
  # Repo email helper compatibility (utility.send_email)
  MAIL_SERVER: ${SMTP_HOST:?SMTP_HOST is required}
  MAIL_PORT: ${SMTP_PORT:?SMTP_PORT is required}
  MAIL_SENDER: ${SMTP_MAIL_FROM:?SMTP_MAIL_FROM is required}
  MAIL_PASSWORD: ${SMTP_PASSWORD:?SMTP_PASSWORD is required}
  MAIL_RECEIVERS: ${ALERT_EMAILS:?ALERT_EMAILS is required}

# Airflow admin bootstrap (used by the one-time `airflow-init` profile).
# Keep password unset here; `airflow-init` will fail fast if it's missing.
x-airflow-admin-env: &airflow-admin-env
  AIRFLOW_ADMIN_USERNAME: ${AIRFLOW_ADMIN_USERNAME:-admin}
  AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD:-}
  AIRFLOW_ADMIN_FIRSTNAME: ${AIRFLOW_ADMIN_FIRSTNAME:-Admin}
  AIRFLOW_ADMIN_LASTNAME: ${AIRFLOW_ADMIN_LASTNAME:-User}
  AIRFLOW_ADMIN_EMAIL: ${AIRFLOW_ADMIN_EMAIL:-admin@example.com}

x-airflow-common: &airflow-common
  image: frog-airflow:2.9.2-python3.12
  build:
    context: .
    dockerfile: airflow.Dockerfile
  user: "${AIRFLOW_UID:-1000}:0"
  environment:
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    # Airflow metadata DB (separate from business data DB)
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://quant:quant@db:5432/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    # Use Redis for Celery task results (DB index 1) to reduce Postgres connection pressure.
    # Business data still writes to Postgres via storage layer; Airflow metadata remains in Postgres.
    AIRFLOW__CELERY__RESULT_BACKEND: redis://redis:6379/1
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: Asia/Shanghai
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
    # Keep this identical across webserver/scheduler/worker, otherwise log fetching can fail with
    # "serve_logs signature was wrong" due to presigned URL verification errors.
    # Override via .env/CI secrets in real deployments.
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY:-frog_airflow_secret_key_change_me}
    AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: "false"
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    # Cap Airflow SQLAlchemy connection usage (metadata DB) to avoid exhausting Postgres connections.
    # These apply to scheduler/webserver/worker processes.
    AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE: "5"
    AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW: "0"
    AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE: "1800"
    AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_PRE_PING: "true"
    # Stopgap: avoid frequent DAG parsing process kills when imports are slow.
    AIRFLOW__SCHEDULER__STALE_DAG_THRESHOLD: "300"
    AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: "120"
    # When worker concurrency < active partitions, tasks can sit in "queued" for hours.
    # Default queued timeout is often too low and causes scheduler to mark them failed.
    AIRFLOW__SCHEDULER__TASK_QUEUED_TIMEOUT: "21600"  # 6 hours
    AIRFLOW__SCHEDULER__TASK_QUEUED_TIMEOUT_CHECK_INTERVAL: "60"
    # Long-running tasks: avoid false-positive zombie detection.
    AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD: "1800"
    # Limit Celery worker concurrency to prevent Postgres connection exhaustion.
    # Each worker process creates its own DB connection pool; too many workers = too many clients.
    AIRFLOW__CELERY__WORKER_CONCURRENCY: "2"
    # Business data DB settings used by storage/storage_db.py (avoid container-local localhost)
    db_host: db
    db_port: 5432
    db_username: quant
    db_password: quant
    # Make project packages (common/, download/, stock/, etc.) importable from DAGs
    PYTHONPATH: /opt/airflow/frog
    FROG_PROJECT_ROOT: /opt/airflow/frog
    # Airflow history-download partitioning (DAG creates up to 16 tasks; this controls active partitions)
    DOWNLOAD_PROCESS_COUNT: ${DOWNLOAD_PROCESS_COUNT:-4}
    # Tushare API token for downloading data (required; define in .env)
    TUSHARE_TOKEN: ${TUSHARE_TOKEN:?TUSHARE_TOKEN is required in .env}
    <<: [*airflow-email-env, *airflow-admin-env]
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./:/opt/airflow/frog
  depends_on: *airflow-common-depends-on

services:
  db:
    image: timescale/timescaledb:latest-pg16
    # Default max_connections for this image can be low; Airflow + workers can briefly spike.
    # Raise it to reduce "too many clients already" failures.
    command: ["postgres", "-c", "max_connections=200"]
    environment:
      <<: *postgres-common-env
    # SECURITY: bind Postgres to localhost only (avoid exposing 5432 to the internet).
    # If you need remote access, prefer VPN/SSH tunnel or a firewall allowlist.
    ports: ["127.0.0.1:5432:5432"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 3s
      retries: 20
    volumes:
      - db_data:/var/lib/postgresql/data
      - ./docker/postgres/init:/docker-entrypoint-initdb.d:ro

  # Ensure the Airflow metadata DB exists on every `docker compose up`.
  # This is necessary because /docker-entrypoint-initdb.d scripts run only
  # on first initialization of the data directory.
  db-init-airflow:
    image: postgres:16-alpine
    depends_on:
      db:
        condition: service_healthy
    environment:
      <<: *postgres-common-env
      AIRFLOW_DB_NAME: airflow
      PGPASSWORD: quant
    restart: "no"
    command:
      - sh
      - -euc
      - |
        until pg_isready -h db -U "$$POSTGRES_USER" -d "$$POSTGRES_DB"; do
          sleep 1;
        done

        exists=$$(psql -h db -U "$$POSTGRES_USER" -d "$$POSTGRES_DB" -tAc "SELECT 1 FROM pg_database WHERE datname='$$AIRFLOW_DB_NAME'" || true)
        if [ "$$exists" != "1" ]; then
          createdb -h db -U "$$POSTGRES_USER" -O "$$POSTGRES_USER" "$$AIRFLOW_DB_NAME"
        fi

  redis:
    image: redis:7-alpine
    # SECURITY: do not publish Redis to the host network.
    # Exposing 6379 publicly allows unauthenticated commands (e.g. REPLICAOF)
    # that can break Celery/Airflow. Containers can still reach Redis via the
    # internal Docker network name "redis".

  app:
    build:
      context: .
      args:
        TUSHARE_TOKEN: ${TUSHARE_TOKEN:?TUSHARE_TOKEN is required in .env}
    depends_on: [db, redis]
    environment:
      DATABASE_URL: postgresql://quant:quant@db:5432/quant
      REDIS_URL: redis://redis:6379/0
      TUSHARE_TOKEN: ${TUSHARE_TOKEN:?TUSHARE_TOKEN is required in .env}
    volumes: [".:/app"]

  airflow-init-permissions:
    image: busybox:latest
    user: root
    volumes:
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./dags:/opt/airflow/dags
    command:
      ["sh", "-c", "mkdir -p /opt/airflow/logs /opt/airflow/plugins /opt/airflow/dags && chown -R ${AIRFLOW_UID:-1000}:0 /opt/airflow/logs /opt/airflow/plugins /opt/airflow/dags && echo 'Permissions fixed successfully!'"]

  airflow-webserver:
    <<: *airflow-common
    ports: ["8080:8080"]
    # Includes init + admin user creation (idempotent) before starting the webserver.
    command:
      ["bash", "-c", "airflow db init && airflow webserver"]

  airflow-scheduler:
    <<: *airflow-common
    command:
      ["bash", "-c", "airflow db upgrade || echo 'DB already upgraded'; airflow scheduler"]
    depends_on:
      <<: *airflow-common-depends-on
      airflow-webserver:
        condition: service_started

  airflow-worker:
    <<: *airflow-common
    command:
      ["bash", "-c", "airflow db upgrade || echo 'DB already upgraded'; airflow celery worker"]
    depends_on:
      <<: *airflow-common-depends-on
      airflow-webserver:
        condition: service_started

  airflow-init:
    <<: *airflow-common
    command:
      ["bash", "-c", "set -euo pipefail; if [ -z \"$$AIRFLOW_ADMIN_PASSWORD\" ]; then echo 'AIRFLOW_ADMIN_PASSWORD is required (set it in .env)'; exit 1; fi; airflow db init; airflow users create --username \"$$AIRFLOW_ADMIN_USERNAME\" --password \"$$AIRFLOW_ADMIN_PASSWORD\" --firstname \"$$AIRFLOW_ADMIN_FIRSTNAME\" --lastname \"$$AIRFLOW_ADMIN_LASTNAME\" --role Admin --email \"$$AIRFLOW_ADMIN_EMAIL\" || echo 'User already exists' "]
    restart: "no"

volumes:
  db_data:
